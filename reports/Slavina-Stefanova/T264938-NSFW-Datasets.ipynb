{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not Safe for Work (NSFW) media Classifier for Wikimedia Commons\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This report addresses the following micro task:\n",
    "\n",
    "* T264938: **Comparison of Existing NSFW Datasets**\n",
    "\n",
    "***\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The aim of this report is to provide an overview of the different available datasets that could be used to train a NSFW classifier. Ideally, the dataset would be balanced between NSFW/SFW, and SFW would contain a proportion of \"difficult\" images, such as different instances of non-sexual nudity: people in bath suits at the beach, face-shots where there's a lot of skin bit no actual inappropriate nudity, etc.\n",
    "\n",
    "Another focus of this report is outlining the ways in which a custom dataset could be built by downloading images of different categories. \n",
    "\n",
    "***\n",
    "\n",
    "## NSFW Datasets\n",
    "\n",
    "### 1. [The Pornography Database (NPDI)](https://sites.google.com/site/pornographydatabase/)\n",
    "\n",
    "> The Pornography database contains nearly 80 hours of 400 pornographic and 400 non-pornographic videos. For the pornographic class, we have browsed websites which only host that kind of material (solving, in a way, the matter of purpose). The database consists of several genres of pornography and depicts actors of many ethnicities, including multi-ethnic ones. For the non-pornographic class, we have browsed general-public purpose video network and selected two samples: 200 videos chosen at random (which we called \"easy\") and 200 videos selected from textual search queries like \"beach\", \"wrestling\", \"swimming\", which we knew would be particularly challenging for the detector (called \"difficult\").\n",
    "\n",
    "This dataset was created by a research group from Brazil and is often mentioned in the literature as a **benchmarking dataset**. However, it's not quite clear whether everyone is using exactly the same images to test on - rather, it would seem that most authors use a random subset of images drawn from this database.\n",
    "\n",
    "All in all, this seems like a solid dataset. I especially like that different ethnicities are represented, as well as several genres of pornography.\n",
    "\n",
    "The major drawback of this dataset is that it's **not openly accessible** - permission needs to be obtained by emailing the authors, and only legal institutions can do so - not individual nor projects, as per their license agreement.\n",
    "\n",
    "I imagine that this dataset consists of frames drawn from the videos mentioned in the description above (rather than just the videos themselves), however this is not quite clear to me.\n",
    "\n",
    "### 2. [Adult Pornography Dataset 2M](http://gvis.unileon.es/dataset/apd-2m/)\n",
    "\n",
    "This is a dataset curated by the University of LÃ©on, Spain. Just like the one above, it's often used for benchmarking purposes (or realistically, a subset thereof). It too can only be obtained through requesting permission:\n",
    "\n",
    ">This dataset is available only for research purposes. Researchers interested in getting this dataset may contact us to request access to this dataset, explaining their contact and the purpose they want the dataset for. We will study each case and send the dataset if it is approved.\n",
    "\n",
    "Nothing further is said about the dataset on the homepage.\n",
    "\n",
    "***\n",
    "\n",
    "## Neutral Datasets\n",
    "\n",
    "Half of the images for our dataset would need to come from a range of SFW categories. As a pre-trained ConvNet probably would be the most likely way to go in terms of classifier design, and most of these architectures have been trained on the ImageNet 1000 classes dataset, I would avoid using samples from this dataset for additional training.\n",
    "\n",
    "There are many ways in which neutral images can be scraped. One option is using tools for batch image downloading, such as those mentioned in the next section. Another way could be using the **Bing Image Search API**. There's of course the possibility to have the neutral images all come directly from Wikimedia Commons. As this task is rather straightforward, I will not get into more detail here.\n",
    "\n",
    "***\n",
    "\n",
    "## Tools & Scripts for data scraping\n",
    "\n",
    "### 1. [Ripme](https://github.com/RipMeApp/ripme)\n",
    "\n",
    "> \"RipMe is an album ripper for various websites. It is a cross-platform tool that runs on your computer, and requires Java 8. RipMe has been tested and confirmed working on Windows, Linux and MacOS.\" \n",
    "\n",
    "Ripme supports a whole range of NSFW websites and subreddits. I clicked on one of the innocent-sounding ones just to check and well... let me just say that I still deeply regret it, several days later.\n",
    "\n",
    "The GUI version is very easy to use. Better still, Ripme it can be run entirely from the CLI.\n",
    "\n",
    "One missing feature is being able to define how many images per Url one wants to download. There's no other option than manually interrupting the process. Limiting the depth of recursive crawling is possible, but depending on the link, this is not always helpful in avoiding getting way too many images. \n",
    "\n",
    "### 2. [nsfw_data_scraper](https://github.com/alex000kim/nsfw_data_scraper)\n",
    "\n",
    ">\"This is a set of scripts that allows for an automatic collection of tens of thousands of images for the following (loosely defined) categories to be later used for training an image classifier:\n",
    ">* porn - pornography images\n",
    ">* hentai - hentai images, but also includes pornographic drawings\n",
    ">* sexy - sexually explicit images, but not pornography. Think nude photos, playboy, bikini, etc.\n",
    ">* neutral - safe for work neutral images of everyday things and people\n",
    ">* drawings - safe for work drawings (including anime)\"\n",
    "\n",
    "Basically, this is a collection of NSFW Urls across different categories and a few scripts that automate downloading the images using Ripme and wget (see further down). \n",
    "\n",
    "### 3. [NSFW data source URLs](https://github.com/EBazarov/nsfw_data_source_urls)\n",
    "\n",
    "> \"Repository contains lists of URLs that will help you download NSFW images, this set can be used in building big enough dataset to train robust NSFW classification model. After downloading and cleaning it's possible to have ~ 500GB or in other words ~ 1 300 000 of NSFW images\"\n",
    "\n",
    "Same as above, but more links and more granular porn categories. Essentially, a collection of naughtu Urls meant to be used with the scripts from nsfw_data_scraper.\n",
    "\n",
    "### 4. [gallery-dl](https://github.com/mikf/gallery-dl)\n",
    "\n",
    ">\"gallery-dl is a command-line program to download image-galleries and -collections from several image hosting sites . It is a cross-platform tool with many configuration options and powerful filenaming capabilities.\"\n",
    "\n",
    "Similar to Ripme, but runs on Python instead of Java and can be easily installed through pip instead of haaving to mess around with Docker. Compared to Ripme, it's got more powerful options, such as being able to limit how many images to download from each Url. \n",
    "\n",
    "[usage](https://www.linuxuprising.com/2019/03/gallery-dl-download-image-galleries-and.html)\n",
    "\n",
    "### 5. [wget](https://www.gnu.org/software/wget/)\n",
    "\n",
    "> \"GNU Wget is a free software package for retrieving files using HTTP, HTTPS, FTP and FTPS, the most widely used Internet protocols. It is a non-interactive commandline tool, so it may easily be called from scripts, cron jobs, terminals without X-Windows support, etc.\"\n",
    "\n",
    "To say this is a very powerful tool is an understatement. Only drawback is that there are so many options and possibilities that the learning curve is a bit steeper. \n",
    "\n",
    "## Libraries for data augmentation\n",
    "\n",
    "### 1. [Augmentor](https://github.com/mdbloice/Augmentor)\n",
    "\n",
    "> \"Augmentor is an image augmentation library in Python for machine learning. It aims to be a standalone library that is platform and framework independent, which is more convenient, allows for finer grained control over augmentation, and implements the most real-world relevant augmentation techniques. It employs a stochastic approach using building blocks that allow for operations to be pieced together in a pipeline.\"\n",
    "\n",
    "In case data augmentation is needed, this is a good Python library to use.\n",
    "\n",
    "***\n",
    "\n",
    "## Other helpful tools\n",
    "\n",
    "### 1.[ Adobe Bridge](https://www.adobe.com/products/bridge.html)\n",
    "\n",
    "> \"Bridge is a powerful creative asset manager that lets you preview, organize, edit, and publish multiple creative assets quickly and easily. Edit metadata. Add keywords, labels, and ratings to assets. Organize assets using collections, and find assets using powerful filters and advanced metadata search features. Collaborate with Libraries and publish to Adobe Stock right from Bridge.\"\n",
    "\n",
    "This is software I have installed on my computer and that I'm already comfortable with. It lets you process huge amounts of images (and also gifs, movies, etc.) very quickly. For instance, you can open a folder of images and quickly look through the thumbnails and delete bad/irrelevant ones etc. using keyboard shortcuts.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of NSFW datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [The Pornography Database (NPDI)](https://sites.google.com/site/pornographydatabase/)\n",
    "**Pros:**\n",
    "- Most commonly used benchmarking dataset in the literature\n",
    "- Different ethnicities are represented\n",
    "- Contains several genres of pornography\n",
    "- Contains both \"easy\" and \"difficult\" images (e.g. non-sexual nudity such as beach photos) \n",
    "\n",
    "**Cons:**\n",
    "- Can only be obtained by requesting permission\n",
    "- Comes from videos; not sure if one would have to extract and sample the frames, or if they are already extracted\n",
    "\n",
    "### 2. [Adult Pornography Dataset 2M](http://gvis.unileon.es/dataset/apd-2m/)\n",
    "**Pros:**\n",
    "- Also commonly used as a benchmarking dataset, albeit less often than the above. \n",
    "- Large: contains 2M images\n",
    "\n",
    "**Cons:**\n",
    "- Can only be obtained by requesting permission, and only available to researchers\n",
    "- Haven't been able to find info about how it was compiled, and what it contains exactly\n",
    "\n",
    "### 3. [nsfw_data_scraper](https://github.com/alex000kim/nsfw_data_scraper)\n",
    "**Pros:**\n",
    "- Big enough for our purposes: one can potentially scrape tens of thousands of images\n",
    "- Divided into various categories, which could prove useful\n",
    "- Easy to download\n",
    "\n",
    "**Cons:**\n",
    "- Noisy, needs to be manually checked for quality \n",
    "- Difficult to know if the dataset is representative of all the porn \"out there\"\n",
    "\n",
    "### 4. [NSFW data source URLs](https://github.com/EBazarov/nsfw_data_source_urls)\n",
    "**Pros:**\n",
    "- Big enough for our purposes: one can potentially scrape 1M+ images\n",
    "- Divided into many subcategories\n",
    "- Easy to download\n",
    "\n",
    "**Cons:**\n",
    "- Noisy, needs to be manually checked for quality\n",
    "- Little variety: all the links are from subreddits and is thus representative of this website's userbase, with the potential bias that entails. \n",
    "- Related to the above, I've looked through around 1000 images that I downloaded by sampling all categories and got the feeling that the data skews rather heavily toward \"young caucasian female posing naked\", with relatively few pictures containing \"people having sex\", or people of other ethnicities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The two \"official\" datasets can only be accessed by requesting permission, which is not infeasible but still a bit of an obstacle. Given the availability of NSFW content on specialized websites/subreddits, I would personally favor curating a custom dataset. \n",
    "\n",
    "Having spent a few hours experimenting with the above-mentioned tools, I now have a pretty good idea of how a pipeline for downloading and organizing the images for such a dataset could be created, with a custom bash script and a little help from Adobe Bridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
